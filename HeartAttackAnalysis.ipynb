{"cells":[{"cell_type":"code","source":["from pyspark.context import SparkContext\nsc=SparkContext.getOrCreate()\n\nheartRdd=sc.textFile(\"/FileStore/tables/data.csv\")\nheartRdd.cache()\nheartRdd.collect()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Remove the first line (contains headers)\nheartRdd = heartRdd.filter(lambda x: \"cp\" not in x)\nheartRdd.count()\nheartRdd.collect()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import math\nfrom pyspark.ml.linalg import Vectors, VectorUDT\n#from pyspark.ml.linalg import Vectors\n\navgChol =sc.broadcast(250)\navgFbs = sc.broadcast(0.06)\navgRrestecg =sc.broadcast(0.21)\navgThalach=sc.broadcast(139.1)\navgExang=sc.broadcast(0.3)\navgSlop=sc.broadcast(1.89)\navgCa=sc.broadcast(0)\navgThal=sc.broadcast(5.6)\navgTres=sc.broadcast(132.5)\n\ndef transformToVector( inputStr) :\n\n    attList=inputStr.split(\",\")    \n    \n    cholValue = attList[4]\n    if cholValue == \"?\":\n        cholValue=avgChol.value\n    fbsValue = attList[5]\n    if fbsValue == \"?\":\n        fbsValue=avgFbs.value\n    restecgValue = attList[6]\n    if restecgValue == \"?\":\n        restecgValue=avgRrestecg.value\n    thalachValue = attList[7]\n    if thalachValue == \"?\":\n        thalachValue=avgThalach.value\n    exangValue = attList[8]\n    if exangValue == \"?\":\n        exangValue=avgExang.value\n    slopValue = attList[10]\n    if slopValue == \"?\":\n        slopValue=avgSlop.value\n    caValue = attList[11]\n    if caValue == \"?\":\n        caValue=avgCa.value\n    thalValue = attList[11]\n    if thalValue == \"?\":\n        thalValue=avgThal.value\n    tresetValue= attList[3]\n    if tresetValue ==\"?\":\n       tresetValue=avgTres.value\n        \n    \n    #Filter out columns not wanted at this stage\n    values= Vectors.dense([float(attList[13]), \\\n                     float(attList[2]),\\\n                     tresetValue,\\\n                     cholValue,  \\\n                     fbsValue,  \\\n                     restecgValue,  \\\n                     thalachValue,  \\\n                     exangValue,  \\\n                     slopValue,  \\\n                     caValue, \\\n                     thalValue \\\n                    ])\n\n  \n    return values\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["heartVectors = heartRdd.map(transformToVector)\nheartVectors.collect()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def transformToLabeledPoint(inStr) :\n    labelPoint = (float(inStr[0]), Vectors.dense([inStr[1],inStr[2],inStr[4]]))\n    return labelPoint\n    \nheartLp = heartVectors.map(transformToLabeledPoint)\nheartDF = sqlContext.createDataFrame(heartLp,[\"label\", \"features\"])\nheartDF.select(\"label\",\"features\").show(10)\n\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["(trainingData, testData) = autoDF.randomSplit([0.9, 0.1])\ntrainingData.count()\n\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["testData.count()\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Setup pipeline\nfrom pyspark.ml.classification import NaiveBayes, NaiveBayesModel\nfrom pyspark.ml import Pipeline\nnbClassifier=NaiveBayes()\npipeline = Pipeline(stages=[nbClassifier])\nmodel = pipeline.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["prediction=model.transform(testData)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["prediction.groupBy(\"label\",\"prediction\").count().show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\ndtClassifer = DecisionTreeClassifier()\ndtModel = dtClassifer.fit(trainingData)\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["prediction1=dtModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["prediction1.groupBy(\"label\",\"prediction\").count().show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"HeartAttackAnalysis","notebookId":2487932735712710},"nbformat":4,"nbformat_minor":0}
